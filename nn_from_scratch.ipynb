{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qiP6_a3bt7jp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om-2W--YcTT1"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, output='softmax', batch_norm = True):\n",
        "        \"\"\"\n",
        "        layer_sizes : list of ints  e.g. [784, 128, 64, 10]\n",
        "        output      : 'softmax'  → multiclass  (use with 'categorical')\n",
        "                      'sigmoid'  → binary       (use with 'binary')\n",
        "                      'linear'   → regression   (use with 'mse')\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        self.output = output\n",
        "        self.batch_norm = batch_norm\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_w = []\n",
        "        self.v_w = []\n",
        "        self.m_b = []\n",
        "        self.v_b = []\n",
        "        self.t = 0\n",
        "        self.gamma = [np.ones((1, size))  for size in layer_sizes[1:-1]]\n",
        "        self.beta  = [np.zeros((1, size)) for size in layer_sizes[1:-1]]\n",
        "        self.running_mean = [np.zeros((1, size)) for size in layer_sizes[1:-1]]\n",
        "        self.running_var  = [np.ones((1, size))  for size in layer_sizes[1:-1]]\n",
        "        self.Z_norm = [np.zeros((1, size)) for size in layer_sizes[1:-1]]\n",
        "        self.Z_var = [np.ones((1, size)) for size in layer_sizes[1:-1]]\n",
        "        self.Z_mean = [np.zeros((1, size)) for size in layer_sizes[1:-1]]\n",
        "        self.dgamma = [np.zeros_like(g) for g in self.gamma]\n",
        "        self.dbeta  = [np.zeros_like(b) for b in self.beta]\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "            self.m_w.append(np.zeros_like(W))\n",
        "            self.v_w.append(np.zeros_like(W))\n",
        "\n",
        "            self.m_b.append(np.zeros_like(b))\n",
        "            self.v_b.append(np.zeros_like(b))\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Activations                                                         #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)   # numerical stability\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # x here is already sigmoid output A, not Z\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Loss                                                                #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def compute_loss(self, y, y_pred, loss='categorical'):\n",
        "        \"\"\"\n",
        "        loss: 'categorical' → softmax output,  one-hot y\n",
        "              'binary'      → sigmoid output,  binary y  (0 or 1)\n",
        "              'mse'         → linear output,   continuous y\n",
        "        \"\"\"\n",
        "        eps = 1e-8\n",
        "        if loss == 'categorical':\n",
        "            y_pred = np.clip(y_pred, eps, 1)\n",
        "            return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
        "\n",
        "        elif loss == 'binary':\n",
        "            y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "            return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "        elif loss == 'mse':\n",
        "            return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss: '{loss}'. Use 'categorical', 'binary', or 'mse'.\")\n",
        "\n",
        "    def batchnorm_forward(self, Z, layer_idx, training=True):\n",
        "      eps = 1e-8\n",
        "      if training:\n",
        "        m = np.mean(Z, axis=0)\n",
        "        v = np.var(Z, axis=0)\n",
        "\n",
        "        self.running_mean[layer_idx] = 0.9 * self.running_mean[layer_idx] + 0.1 * m\n",
        "        self.running_var[layer_idx] = 0.9 * self.running_var[layer_idx] + 0.1 * v\n",
        "      else:\n",
        "        m = self.running_mean[layer_idx]\n",
        "        v = self.running_var[layer_idx]\n",
        "\n",
        "      Z_norm = (Z - m) / np.sqrt(v + eps)\n",
        "      Z_out = self.gamma[layer_idx] * Z_norm + self.beta[layer_idx]\n",
        "\n",
        "      self.Z_norm[layer_idx] = Z_norm\n",
        "      self.Z_var[layer_idx]  = v\n",
        "      self.Z_mean[layer_idx] = m\n",
        "\n",
        "      return Z_out\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Forward                                                             #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self, X, training=True):\n",
        "        \"\"\"\n",
        "        Hidden layers  → always ReLU\n",
        "        Output layer   → determined by self.output ('softmax', 'sigmoid', 'linear')\n",
        "        \"\"\"\n",
        "        self.A = [X]\n",
        "        self.Z = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = self.A[i] @ self.weights[i] + self.biases[i]\n",
        "\n",
        "            if i == len(self.weights) - 1:          # output layer\n",
        "                if self.output == 'softmax':\n",
        "                    a = self.softmax(z)\n",
        "                elif self.output == 'sigmoid':\n",
        "                    a = self.sigmoid(z)\n",
        "                else:                               # linear\n",
        "                    a = z\n",
        "            else:                                   # hidden layers\n",
        "              if self.batch_norm:\n",
        "                z = self.batchnorm_forward(z, layer_idx=i, training=training)\n",
        "              a = self.relu(z)\n",
        "\n",
        "            self.Z.append(z)\n",
        "            self.A.append(a)\n",
        "\n",
        "        return self.A[-1]\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Backward                                                            #\n",
        "    # ------------------------------------------------------------------ #\n",
        "\n",
        "    def batchnorm_backward(self, dZ_bn, layer_idx):\n",
        "      eps   = 1e-8\n",
        "      n     = dZ_bn.shape[0]\n",
        "      Z_norm = self.Z_norm[layer_idx]\n",
        "      var    = self.Z_var[layer_idx]\n",
        "      mean   = self.Z_mean[layer_idx]\n",
        "      Z      = Z_norm * np.sqrt(var + eps) + mean  # recover original Z\n",
        "\n",
        "      # gradients for gamma and beta\n",
        "      dgamma = np.sum(dZ_bn * Z_norm, axis=0, keepdims=True)\n",
        "      dbeta  = np.sum(dZ_bn, axis=0, keepdims=True)\n",
        "\n",
        "      # store for update later\n",
        "      self.dgamma[layer_idx] = dgamma\n",
        "      self.dbeta[layer_idx]  = dbeta\n",
        "\n",
        "      # gradient to pass back through normalization\n",
        "      dZ_norm = dZ_bn * self.gamma[layer_idx]\n",
        "      dvar    = np.sum(dZ_norm * (Z - mean) * -0.5 * (var + eps) ** -1.5, axis=0, keepdims=True)\n",
        "      dmean   = np.sum(dZ_norm * -1 / np.sqrt(var + eps), axis=0, keepdims=True)\n",
        "      dZ      = dZ_norm / np.sqrt(var + eps) + dvar * 2 * (Z - mean) / n + dmean / n\n",
        "\n",
        "      return dZ\n",
        "\n",
        "    def backward(self, y, optimizer='Adam', lr=0.001):\n",
        "        \"\"\"\n",
        "        Output layer gradient depends on output activation + loss pairing:\n",
        "          softmax  + categorical → dZ = A - y   (simplified combined derivative)\n",
        "          sigmoid  + binary      → dZ = A - y   (also simplifies cleanly)\n",
        "          linear   + mse         → dZ = A - y   (MSE derivative, no activation to chain)\n",
        "\n",
        "        All three simplify to the same thing — A - y.\n",
        "        Hidden layers always use ReLU derivative.\n",
        "        \"\"\"\n",
        "        n  = y.shape[0]\n",
        "        L  = len(self.weights)\n",
        "        self.t += 1\n",
        "        b1, b2, eps = 0.9, 0.999, 1e-8\n",
        "\n",
        "        # output layer — all three pairings simplify to A - y\n",
        "        dA = self.A[-1] - y\n",
        "\n",
        "        for i in reversed(range(L)):\n",
        "            if i == L - 1:\n",
        "                dZ = dA                                     # output layer\n",
        "            else:\n",
        "                dZ = dA * self.relu_derivative(self.Z[i])  # hidden layers\n",
        "                if self.batch_norm:\n",
        "                  dZ = self.batchnorm_backward(dZ, layer_idx=i)\n",
        "\n",
        "            dW = self.A[i].T @ dZ / n\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / n\n",
        "            dA = dZ @ self.weights[i].T                     # pass gradient back\n",
        "            if optimizer == 'Adam':\n",
        "              self.m_w[i] = b1 * self.m_w[i] + (1 - b1) * dW\n",
        "              self.v_w[i] = b2 * self.v_w[i] + (1 - b2) * (dW ** 2)\n",
        "              m_w_hat = self.m_w[i] / (1 - b1 ** self.t)\n",
        "              v_w_hat = self.v_w[i] / (1 - b2 ** self.t)\n",
        "              self.m_b[i] = b1 * self.m_b[i] + (1 - b1) * db\n",
        "              self.v_b[i] = b2 * self.v_b[i] + (1 - b2) * (db ** 2)\n",
        "              m_b_hat = self.m_b[i] / (1 - b1 ** self.t)\n",
        "              v_b_hat = self.v_b[i] / (1 - b2 ** self.t)\n",
        "              self.weights[i] -= lr * (m_w_hat / (np.sqrt(v_w_hat) + eps))\n",
        "              self.biases[i]  -= lr * (m_b_hat / (np.sqrt(v_b_hat) + eps))\n",
        "            else:\n",
        "              self.weights[i] -= lr * dW\n",
        "              self.biases[i]  -= lr * db\n",
        "        # after the loop — update gamma and beta same as weights\n",
        "        if self.batch_norm:  \n",
        "          for i in range(len(self.gamma)):\n",
        "              self.gamma[i] -= lr * self.dgamma[i]\n",
        "              self.beta[i]  -= lr * self.dbeta[i]\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Helpers                                                             #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def to_onehot(self, y, num_classes):\n",
        "        one_hot = np.zeros((y.size, num_classes))\n",
        "        one_hot[np.arange(y.size), y.flatten()] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def sample_data(self, mode='binary'):\n",
        "        \"\"\"\n",
        "        XOR dataset.\n",
        "        mode: 'binary'      → y shape (4,1)  for sigmoid + binary loss\n",
        "              'categorical' → y shape (4,2)  for softmax + categorical loss\n",
        "        \"\"\"\n",
        "        X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
        "        y = np.array([[0],[1],[1],[0]])\n",
        "        if mode == 'binary':\n",
        "            return X, y\n",
        "        else:\n",
        "            return X, self.to_onehot(y, num_classes=2)\n",
        "\n",
        "    def lr_decay(self, lr, epoch, mode='none', decay_rate=0.5, step_size=10):\n",
        "      if mode == 'none':\n",
        "          return lr\n",
        "      elif mode == 'step':\n",
        "          if epoch % step_size == 0 and epoch > 0:   # ← boundary check lives here\n",
        "              return lr * decay_rate\n",
        "          return lr                                   # ← unchanged otherwise\n",
        "      elif mode == 'exponential':\n",
        "          return lr * (decay_rate ** epoch)\n",
        "      elif mode == '1/t':\n",
        "          return lr / (1 + decay_rate * epoch)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Train                                                               #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def train(self, X, y, epochs=20, lr=0.001, batch_size=64, \n",
        "          loss='categorical', optimizer='Adam', verbose=True,\n",
        "          lr_decay='none', decay_rate=0.5, step_size=10):\n",
        "        self.t = 0\n",
        "        n = X.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(n)\n",
        "            X, y = X[indices], y[indices]\n",
        "\n",
        "            epoch_loss  = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            current_lr = self.lr_decay(lr, epoch=epoch, \n",
        "                                       mode=lr_decay, decay_rate=decay_rate, \n",
        "                                       step_size=step_size)\n",
        "\n",
        "            for start in range(0, n, batch_size):\n",
        "              X_batch = X[start:start + batch_size]\n",
        "              y_batch = y[start:start + batch_size]\n",
        "\n",
        "              y_pred = self.forward(X_batch, training=True)\n",
        "              epoch_loss += self.compute_loss(y_batch, y_pred, loss=loss)\n",
        "              self.backward(y_batch, lr=current_lr, optimizer=optimizer)\n",
        "              num_batches += 1\n",
        "\n",
        "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):\n",
        "                avg_loss = epoch_loss / num_batches\n",
        "                print(f\"Epoch {epoch+1:>4}/{epochs}  |  Loss: {avg_loss:.4f}  |  LR: {current_lr}\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Evaluate & Predict                                                  #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def evaluate(self, X, y, loss='categorical'):\n",
        "        \"\"\"Returns accuracy for classification, MSE for regression.\"\"\"\n",
        "        y_pred = self.forward(X, training=False)\n",
        "        if loss == 'categorical':\n",
        "            return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
        "        elif loss == 'binary':\n",
        "            return np.mean(np.round(y_pred) == y)\n",
        "        elif loss == 'mse':\n",
        "            return self.compute_loss(y, y_pred, loss='mse')\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X, training=False)\n",
        "        if self.output == 'softmax':\n",
        "            return np.argmax(y_pred, axis=1)\n",
        "        elif self.output == 'sigmoid':\n",
        "            return np.round(y_pred)\n",
        "        else:\n",
        "            return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un2VFJ7GuM0X"
      },
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk3Oj2lPCRG8",
        "outputId": "6d33d776-bf24-4f99-dfca-e045c5ae054b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=======================================================\n",
            "TEST 1 — XOR  |  sigmoid + binary loss\n",
            "=======================================================\n",
            "Predictions : [1, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "Accuracy    : 75.0%\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 55)\n",
        "print(\"TEST 1 — XOR  |  sigmoid + binary loss\")\n",
        "print(\"=\" * 55)\n",
        "nn1 = NeuralNetwork([2, 4, 1], output='sigmoid')\n",
        "X, y = nn1.sample_data(mode='binary')\n",
        "nn1.train(X, y, epochs=5000, lr=0.001, batch_size=6, loss='binary', verbose=False)\n",
        "preds = nn1.predict(X)\n",
        "acc   = nn1.evaluate(X, y, loss='binary')\n",
        "print(f\"Predictions : {preds.flatten().astype(int).tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"Accuracy    : {acc * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvACCl0XYMer",
        "outputId": "986cee0d-2274-416c-87fa-8c4803aa62aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=======================================================\n",
            "TEST 2 — XOR  |  softmax + categorical loss\n",
            "=======================================================\n",
            "Predictions : [0, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "Accuracy    : 100.0%\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 2 — XOR  |  softmax + categorical loss\")\n",
        "print(\"=\" * 55)\n",
        "nn2 = NeuralNetwork([2, 4, 2], output='softmax')\n",
        "X, y = nn2.sample_data(mode='categorical')\n",
        "nn2.train(X, y, epochs=5000, lr=0.1, batch_size=4, loss='categorical', verbose=False)\n",
        "preds = nn2.predict(X)\n",
        "acc   = nn2.evaluate(X, y, loss='categorical')\n",
        "print(f\"Predictions : {preds.tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"Accuracy    : {acc * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kx-YkuvuIHM",
        "outputId": "0eaff032-5e66-4f06-864d-db2f9bdfe45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=======================================================\n",
            "TEST 3 — XOR  |  MSE loss  (regression-style)\n",
            "=======================================================\n",
            "Raw outputs : [0.666, 0.666, 0.666, 0.0]\n",
            "Rounded     : [1, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "MSE         : 0.1667\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 3 — XOR  |  MSE loss  (regression-style)\")\n",
        "print(\"=\" * 55)\n",
        "nn3 = NeuralNetwork([2, 4, 1], output='linear')\n",
        "X, y = nn3.sample_data(mode='binary')\n",
        "y = y.astype(float)\n",
        "nn3.train(X, y, epochs=5000, lr=0.001, batch_size=4, loss='mse', verbose=False)\n",
        "preds = nn3.predict(X)\n",
        "mse   = nn3.evaluate(X, y, loss='mse')\n",
        "print(f\"Raw outputs : {preds.flatten().round(3).tolist()}\")\n",
        "print(f\"Rounded     : {np.round(preds).flatten().astype(int).tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"MSE         : {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LcFbMoAuKF8",
        "outputId": "979460c9-bdcc-471c-ee38-583c9f103d9f"
      },
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 4 — MNIST  |  softmax + categorical loss\")\n",
        "print(\"=\" * 55)\n",
        "print(\"Loading MNIST...\")\n",
        "X_mnist, y_mnist = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "X_mnist = X_mnist / 255.0\n",
        "y_mnist = y_mnist.astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_mnist, y_mnist, test_size=10000, random_state=42\n",
        ")\n",
        "print(\"MNIST Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn4 = NeuralNetwork([784, 128, 64, 10], output='softmax', batch_norm=True)\n",
        "y_train_oh = nn4.to_onehot(y_train, 10)\n",
        "y_test_oh  = nn4.to_onehot(y_test,  10)\n",
        "print(f\"Training on {X_train.shape[0]} samples...\\n\")\n",
        "nn4.train(\n",
        "    X_train, y_train_oh,\n",
        "    epochs=20, lr=0.001, batch_size=64,\n",
        "    loss='categorical', optimizer='Adam',\n",
        "    lr_decay='exponential')\n",
        "test_acc = nn4.evaluate(X_test, y_test_oh, loss='categorical')\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
