{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qiP6_a3bt7jp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, output='softmax'):\n",
        "        \"\"\"\n",
        "        layer_sizes : list of ints  e.g. [784, 128, 64, 10]\n",
        "        output      : 'softmax'  → multiclass  (use with 'categorical')\n",
        "                      'sigmoid'  → binary       (use with 'binary')\n",
        "                      'linear'   → regression   (use with 'mse')\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        self.output = output\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_w = []\n",
        "        self.v_w = []\n",
        "        self.m_b = []\n",
        "        self.v_b = []\n",
        "        self.t = 0\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            W = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i])\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "            self.m_w.append(np.zeros_like(W))\n",
        "            self.v_w.append(np.zeros_like(W))\n",
        "\n",
        "            self.m_b.append(np.zeros_like(b))\n",
        "            self.v_b.append(np.zeros_like(b))\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Activations                                                         #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)   # numerical stability\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        # x here is already sigmoid output A, not Z\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Loss                                                                #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def compute_loss(self, y, y_pred, loss='categorical'):\n",
        "        \"\"\"\n",
        "        loss: 'categorical' → softmax output,  one-hot y\n",
        "              'binary'      → sigmoid output,  binary y  (0 or 1)\n",
        "              'mse'         → linear output,   continuous y\n",
        "        \"\"\"\n",
        "        eps = 1e-8\n",
        "        if loss == 'categorical':\n",
        "            y_pred = np.clip(y_pred, eps, 1)\n",
        "            return -np.mean(np.sum(y * np.log(y_pred), axis=1))\n",
        "\n",
        "        elif loss == 'binary':\n",
        "            y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "            return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "\n",
        "        elif loss == 'mse':\n",
        "            return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss: '{loss}'. Use 'categorical', 'binary', or 'mse'.\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Forward                                                             #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Hidden layers  → always ReLU\n",
        "        Output layer   → determined by self.output ('softmax', 'sigmoid', 'linear')\n",
        "        \"\"\"\n",
        "        self.A = [X]\n",
        "        self.Z = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = self.A[i] @ self.weights[i] + self.biases[i]\n",
        "            self.Z.append(z)\n",
        "\n",
        "            if i == len(self.weights) - 1:          # output layer\n",
        "                if self.output == 'softmax':\n",
        "                    a = self.softmax(z)\n",
        "                elif self.output == 'sigmoid':\n",
        "                    a = self.sigmoid(z)\n",
        "                else:                               # linear\n",
        "                    a = z\n",
        "            else:                                   # hidden layers\n",
        "                a = self.relu(z)\n",
        "\n",
        "            self.A.append(a)\n",
        "\n",
        "        return self.A[-1]\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Backward                                                            #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def backward(self, y, optimizer='Adam', lr=0.001):\n",
        "        \"\"\"\n",
        "        Output layer gradient depends on output activation + loss pairing:\n",
        "          softmax  + categorical → dZ = A - y   (simplified combined derivative)\n",
        "          sigmoid  + binary      → dZ = A - y   (also simplifies cleanly)\n",
        "          linear   + mse         → dZ = A - y   (MSE derivative, no activation to chain)\n",
        "\n",
        "        All three simplify to the same thing — A - y.\n",
        "        Hidden layers always use ReLU derivative.\n",
        "        \"\"\"\n",
        "        n  = y.shape[0]\n",
        "        L  = len(self.weights)\n",
        "        self.t += 1\n",
        "        b1, b2, eps = 0.9, 0.999, 1e-8\n",
        "\n",
        "        # output layer — all three pairings simplify to A - y\n",
        "        dA = self.A[-1] - y\n",
        "\n",
        "        for i in reversed(range(L)):\n",
        "            if i == L - 1:\n",
        "                dZ = dA                                     # output layer\n",
        "            else:\n",
        "                dZ = dA * self.relu_derivative(self.Z[i])  # hidden layers\n",
        "\n",
        "            dW = self.A[i].T @ dZ / n\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / n\n",
        "            dA = dZ @ self.weights[i].T                     # pass gradient back\n",
        "            if optimizer == 'Adam':\n",
        "              self.m_w[i] = b1 * self.m_w[i] + (1 - b1) * dW\n",
        "              self.v_w[i] = b2 * self.v_w[i] + (1 - b2) * (dW ** 2)\n",
        "              m_w_hat = self.m_w[i] / (1 - b1 ** self.t)\n",
        "              v_w_hat = self.v_w[i] / (1 - b2 ** self.t)\n",
        "              self.m_b[i] = b1 * self.m_b[i] + (1 - b1) * db\n",
        "              self.v_b[i] = b2 * self.v_b[i] + (1 - b2) * (db ** 2)\n",
        "              m_b_hat = self.m_b[i] / (1 - b1 ** self.t)\n",
        "              v_b_hat = self.v_b[i] / (1 - b2 ** self.t)\n",
        "              self.weights[i] -= lr * (m_w_hat / (np.sqrt(v_w_hat) + eps))\n",
        "              self.biases[i]  -= lr * (m_b_hat / (np.sqrt(v_b_hat) + eps))\n",
        "            else:\n",
        "              self.weights[i] -= lr * dW\n",
        "              self.biases[i]  -= lr * db\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Helpers                                                             #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def to_onehot(self, y, num_classes):\n",
        "        one_hot = np.zeros((y.size, num_classes))\n",
        "        one_hot[np.arange(y.size), y.flatten()] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def sample_data(self, mode='binary'):\n",
        "        \"\"\"\n",
        "        XOR dataset.\n",
        "        mode: 'binary'      → y shape (4,1)  for sigmoid + binary loss\n",
        "              'categorical' → y shape (4,2)  for softmax + categorical loss\n",
        "        \"\"\"\n",
        "        X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n",
        "        y = np.array([[0],[1],[1],[0]])\n",
        "        if mode == 'binary':\n",
        "            return X, y\n",
        "        else:\n",
        "            return X, self.to_onehot(y, num_classes=2)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Train                                                               #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def train(self, X, y, epochs=20, lr=0.001, batch_size=64, loss='categorical', optimizer='Adam', verbose=True):\n",
        "        self.t = 0\n",
        "        n = X.shape[0]\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(n)\n",
        "            X, y = X[indices], y[indices]\n",
        "\n",
        "            epoch_loss  = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            for start in range(0, n, batch_size):\n",
        "                X_batch = X[start:start + batch_size]\n",
        "                y_batch = y[start:start + batch_size]\n",
        "\n",
        "                y_pred = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, y_pred, loss=loss)\n",
        "                self.backward(y_batch, lr=lr, optimizer=optimizer)\n",
        "                num_batches += 1\n",
        "\n",
        "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):\n",
        "                avg_loss = epoch_loss / num_batches\n",
        "                print(f\"Epoch {epoch+1:>4}/{epochs}  |  Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    #  Evaluate & Predict                                                  #\n",
        "    # ------------------------------------------------------------------ #\n",
        "    def evaluate(self, X, y, loss='categorical'):\n",
        "        \"\"\"Returns accuracy for classification, MSE for regression.\"\"\"\n",
        "        y_pred = self.forward(X)\n",
        "        if loss == 'categorical':\n",
        "            return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n",
        "        elif loss == 'binary':\n",
        "            return np.mean(np.round(y_pred) == y)\n",
        "        elif loss == 'mse':\n",
        "            return self.compute_loss(y, y_pred, loss='mse')\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        if self.output == 'softmax':\n",
        "            return np.argmax(y_pred, axis=1)\n",
        "        elif self.output == 'sigmoid':\n",
        "            return np.round(y_pred)\n",
        "        else:\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "Om-2W--YcTT1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests"
      ],
      "metadata": {
        "id": "Un2VFJ7GuM0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 55)\n",
        "print(\"TEST 1 — XOR  |  sigmoid + binary loss\")\n",
        "print(\"=\" * 55)\n",
        "nn1 = NeuralNetwork([2, 4, 1], output='sigmoid')\n",
        "X, y = nn1.sample_data(mode='binary')\n",
        "nn1.train(X, y, epochs=5000, lr=0.001, batch_size=6, loss='binary', verbose=False)\n",
        "preds = nn1.predict(X)\n",
        "acc   = nn1.evaluate(X, y, loss='binary')\n",
        "print(f\"Predictions : {preds.flatten().astype(int).tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"Accuracy    : {acc * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk3Oj2lPCRG8",
        "outputId": "6d33d776-bf24-4f99-dfca-e045c5ae054b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "TEST 1 — XOR  |  sigmoid + binary loss\n",
            "=======================================================\n",
            "Predictions : [1, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "Accuracy    : 75.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 2 — XOR  |  softmax + categorical loss\")\n",
        "print(\"=\" * 55)\n",
        "nn2 = NeuralNetwork([2, 4, 2], output='softmax')\n",
        "X, y = nn2.sample_data(mode='categorical')\n",
        "nn2.train(X, y, epochs=5000, lr=0.1, batch_size=4, loss='categorical', verbose=False)\n",
        "preds = nn2.predict(X)\n",
        "acc   = nn2.evaluate(X, y, loss='categorical')\n",
        "print(f\"Predictions : {preds.tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"Accuracy    : {acc * 100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvACCl0XYMer",
        "outputId": "986cee0d-2274-416c-87fa-8c4803aa62aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "TEST 2 — XOR  |  softmax + categorical loss\n",
            "=======================================================\n",
            "Predictions : [0, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "Accuracy    : 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 3 — XOR  |  MSE loss  (regression-style)\")\n",
        "print(\"=\" * 55)\n",
        "nn3 = NeuralNetwork([2, 4, 1], output='linear')\n",
        "X, y = nn3.sample_data(mode='binary')\n",
        "y = y.astype(float)\n",
        "nn3.train(X, y, epochs=5000, lr=0.001, batch_size=4, loss='mse', verbose=False)\n",
        "preds = nn3.predict(X)\n",
        "mse   = nn3.evaluate(X, y, loss='mse')\n",
        "print(f\"Raw outputs : {preds.flatten().round(3).tolist()}\")\n",
        "print(f\"Rounded     : {np.round(preds).flatten().astype(int).tolist()}\")\n",
        "print(f\"Expected    : [0, 1, 1, 0]\")\n",
        "print(f\"MSE         : {mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kx-YkuvuIHM",
        "outputId": "0eaff032-5e66-4f06-864d-db2f9bdfe45c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "TEST 3 — XOR  |  MSE loss  (regression-style)\n",
            "=======================================================\n",
            "Raw outputs : [0.666, 0.666, 0.666, 0.0]\n",
            "Rounded     : [1, 1, 1, 0]\n",
            "Expected    : [0, 1, 1, 0]\n",
            "MSE         : 0.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()\n",
        "print(\"=\" * 55)\n",
        "print(\"TEST 4 — MNIST  |  softmax + categorical loss\")\n",
        "print(\"=\" * 55)\n",
        "print(\"Loading MNIST...\")\n",
        "X_mnist, y_mnist = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "X_mnist = X_mnist / 255.0\n",
        "y_mnist = y_mnist.astype(int)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_mnist, y_mnist, test_size=10000, random_state=42\n",
        ")\n",
        "nn4 = NeuralNetwork([784, 128, 64, 10], output='softmax')\n",
        "y_train_oh = nn4.to_onehot(y_train, 10)\n",
        "y_test_oh  = nn4.to_onehot(y_test,  10)\n",
        "print(f\"Training on {X_train.shape[0]} samples...\\n\")\n",
        "nn4.train(\n",
        "    X_train, y_train_oh,\n",
        "    epochs=20, lr=0.001, batch_size=64,\n",
        "    loss='categorical', optimizer='Adam')\n",
        "test_acc = nn4.evaluate(X_test, y_test_oh, loss='categorical')\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LcFbMoAuKF8",
        "outputId": "979460c9-bdcc-471c-ee38-583c9f103d9f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "TEST 4 — MNIST  |  softmax + categorical loss\n",
            "=======================================================\n",
            "Loading MNIST...\n",
            "Training on 60000 samples...\n",
            "\n",
            "Epoch    1/20  |  Loss: 0.2691\n",
            "Epoch    3/20  |  Loss: 0.0781\n",
            "Epoch    5/20  |  Loss: 0.0439\n",
            "Epoch    7/20  |  Loss: 0.0311\n",
            "Epoch    9/20  |  Loss: 0.0216\n",
            "Epoch   11/20  |  Loss: 0.0160\n",
            "Epoch   13/20  |  Loss: 0.0114\n",
            "Epoch   15/20  |  Loss: 0.0103\n",
            "Epoch   17/20  |  Loss: 0.0102\n",
            "Epoch   19/20  |  Loss: 0.0108\n",
            "Epoch   20/20  |  Loss: 0.0082\n",
            "\n",
            "Final Test Accuracy: 97.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnQ4OsKfuozN"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}